<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/blog-logo-32px.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/blog-logo-16px.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <link rel="manifest" href="/images/manifest.json">
  <meta name="msapplication-config" content="/images/browserconfig.xml">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yangsuoly.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":true,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="9. 新的模型架构 通过前文的学习，我们知道神经预言模型的核心借口是一个将 token 序列映射到上下文嵌入的编码器：  \text{the}, \text{mouse}, \text{ate}, \text{the}, \text{cheese}] \stackrel{\phi}{\Rightarrow} \left[\binom{1}{0.1}, \binom{0}{1}, \binom{1}">
<meta property="og:type" content="article">
<meta property="og:title" content="Large-lm-7-Mixed-Framework">
<meta property="og:url" content="http://yangsuoly.com/2023/09/16/Large-lm-7-Mixed-Framework/index.html">
<meta property="og:site_name" content="独孤诗人的学习驿站">
<meta property="og:description" content="9. 新的模型架构 通过前文的学习，我们知道神经预言模型的核心借口是一个将 token 序列映射到上下文嵌入的编码器：  \text{the}, \text{mouse}, \text{ate}, \text{the}, \text{cheese}] \stackrel{\phi}{\Rightarrow} \left[\binom{1}{0.1}, \binom{0}{1}, \binom{1}">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/YangSuoly/Images/RL-notes/202309181209741.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/YangSuoly/Images/RL-notes/202309181225873.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/YangSuoly/Images/RL-notes/202309181646256.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/YangSuoly/Images/RL-notes/202309181821233.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/YangSuoly/Images/RL-notes/202309190852239.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/YangSuoly/Images/RL-notes/202309190852619.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/YangSuoly/Images/RL-notes/202309190854778.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/YangSuoly/Images/RL-notes/202309190922402.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/YangSuoly/Images/RL-notes/202309191001403.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/YangSuoly/Images/RL-notes/202309191020000.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/YangSuoly/Images/RL-notes/202309191025429.png">
<meta property="article:published_time" content="2023-09-16T09:37:00.000Z">
<meta property="article:modified_time" content="2023-09-19T03:23:50.957Z">
<meta property="article:author" content="YangSu">
<meta property="article:tag" content="LM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/YangSuoly/Images/RL-notes/202309181209741.png">

<link rel="canonical" href="http://yangsuoly.com/2023/09/16/Large-lm-7-Mixed-Framework/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Large-lm-7-Mixed-Framework | 独孤诗人的学习驿站</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?f59f5fb59d32ec3903088b0f976956d1";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>
<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta custom-logo">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">独孤诗人的学习驿站</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">生活就是一半烟火，一半诗意。手执烟火谋生活，心怀诗意谋未来……</p>
      <a>
        <img class="custom-logo-image" src="/images/logo@2x.png" alt="独孤诗人的学习驿站">
      </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/Tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/Categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/YangSuoly" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yangsuoly.com/2023/09/16/Large-lm-7-Mixed-Framework/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/YangSu.jpg">
      <meta itemprop="name" content="YangSu">
      <meta itemprop="description" content="A blog for recording learning notes...">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="独孤诗人的学习驿站">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Large-lm-7-Mixed-Framework
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-09-16 17:37:00" itemprop="dateCreated datePublished" datetime="2023-09-16T17:37:00+08:00">2023-09-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-09-19 11:23:50" itemprop="dateModified" datetime="2023-09-19T11:23:50+08:00">2023-09-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/Categories/DataWhale/" itemprop="url" rel="index"><span itemprop="name">DataWhale</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/Categories/DataWhale/Large-LM/" itemprop="url" rel="index"><span itemprop="name">Large LM</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>9.6k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>9 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="新的模型架构">9. 新的模型架构</h1>
<p>通过前文的学习，我们知道神经预言模型的核心借口是一个将
<code>token</code> 序列映射到上下文嵌入的编码器： <span
class="math display">
\text{the}, \text{mouse}, \text{ate}, \text{the}, \text{cheese}]
\stackrel{\phi}{\Rightarrow} \left[\binom{1}{0.1}, \binom{0}{1},
\binom{1}{1}, \binom{1}{-0.1}, \binom{0}{-1} \right].
</span> <a id="more"></a></p>
<p>以 <code>GPT-3</code> 为例，它是一个通过堆叠 <code>96</code> 层
Transformer block，映射 <code>token</code> 序列 <span
class="math inline">x_{1:L}</span> 的神经语言模型：</p>
<p><span class="math display">
\text{GPT-3}(x_{1:L}) =
\text{TransformerBlock}^{96}(\text{EmbedTokenWithPosition}(x_{1:L})),
</span></p>
<p>其中，每层Transformer block使用</p>
<ul>
<li>自注意力层，允许每个<code>token</code> 进行交互；</li>
<li>前馈层，独立处理每个 <code>token</code>。</li>
</ul>
<p>这种稠密的 <code>Transformer</code>
模型架构是目前开发大语言模型的主要范式。但是，扩展这种模型并非易事，需要数据、模型和流水并行。</p>
<p>现状：</p>
<ul>
<li><p>我们的规模已经到了极限。</p></li>
<li><p>随着模型越来越大，它们必须被拆分到更多的机器上。下面是一个模型并行示例（具体细节见上一章）：</p>
<p><span class="math display">
\begin{align*}
\text{GPU1}[\text{layer1}, \text{layer2}]  \\
\text{GPU2}[\text{layer3}, \text{layer4}]  \\
\text{GPU3}[\text{layer5}, \text{layer6}]
\end{align*}
</span></p></li>
</ul>
<p>因此，如果我们要继续扩大规模，我们需要重新思考如何构建大语言模型。对于稠密的
<code>Transformer</code>
模型，每个输入使用语言模型的相同（所有）参数（如 <code>GPT-3</code> 的
<code>175B</code> 参数）。</p>
<p>相反，我们是否可以让每个输入使用不同的（更小的）参数子集？在本章中，我们将探讨两种不同类型的“新”模型架构，这提高了模型的规模上限。特别地，我们将讨论：</p>
<ul>
<li><p>混合专家模型：创建一组专家，每个输入只激活一小部分专家。</p>
<ul>
<li>直觉：类似于咨询委员会，每个专家有不同的背景（如历史、数学、科学等）。</li>
</ul>
<p><span class="math display">
\text{input} \quad\quad\Rightarrow\quad\quad \text{expert}_1 \quad
\text{expert}_2 \quad \text{expert}_3 \quad \text{expert}_4
\quad\quad\Rightarrow\quad\quad \text{output}.
</span></p></li>
<li><p>基于检索的模型：借助原始数据存储库，给定新输入，从中检索与它相关的部分，并使用它们来预测输出。</p>
<ul>
<li>直觉：借助网络搜索查询问题的答案。</li>
</ul>
<p><span class="math display">
\text{store} \quad\quad|\quad\quad \text{input}
\quad\quad\Rightarrow\quad\quad \text{relevant data from store} \quad
\quad\quad\Rightarrow\quad\quad \text{output}.
</span></p></li>
</ul>
<h2 id="混合专家模型">9.1 混合专家模型</h2>
<h3 id="基础知识">9.1.1 基础知识</h3>
<p>混合专家的想法可以追溯到 <a
target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~fritz/absps/jjnh91.pdf">Jacobs et al.
(1991)</a>。</p>
<p><img src="https://cdn.jsdelivr.net/gh/YangSuoly/Images/RL-notes/202309181209741.png" style="zoom:80%;" /></p>
<p>以预测问题为例： <span class="math display">
x \in \mathbb{R}^d \Rightarrow y \in \mathbb{R}^d.
</span></p>
<p>让我们从学习前馈（ReLU）神经网络开始：</p>
<p><span class="math display">
h_\theta(x) = W_2 \max(W_1 x, 0),
</span></p>
<p>其中参数为 <span class="math inline">\theta = (W_1,
W_2)</span>。然而，这个函数可能表达能力不足。因此，可以使神经网络更宽或更深。</p>
<p>专家的混合方法步骤如下：</p>
<ul>
<li><p>定义 <span class="math inline">E</span> 个专家。</p></li>
<li><p>每个专家 <span class="math inline">e = 1, \dots, E</span>
都具有自己的嵌入 <span class="math inline">w_e \in
\mathbb{R}^d</span>。</p></li>
<li><p>将门控函数定义为 <span class="math inline">E</span>
个专家上的概率分布：</p>
<p><span class="math display">
g_e(x) = \frac{\exp(w_e \cdot x)}{\sum_{e&#39; = 1}^E \exp(w_{e&#39;}
\cdot x)}.
</span></p></li>
<li><p>每个专家 <span class="math inline">e = 1, \dots, E</span>
都具有自己的参数 <span class="math inline">\theta^{(e)} = (W_1^{(e)},
W_2^{(e)})</span>。</p></li>
<li><p>根据专家特定参数定义每个专家函数：</p>
<p><span class="math display">
h_{\theta_e}(x) = W_2^{(e)} \max(W_1^{(e)} x, 0).
</span></p></li>
<li><p>将最终函数定义为专家的混合：</p>
<p><span class="math display">
f(x) = \sum_{e=1}^E \underbrace{g_e(x)}_\text{gating}
\underbrace{h_{\theta_e}(x)}_\text{expert}.
</span></p></li>
</ul>
<p><strong>示例</strong></p>
<p>考虑d=2，并且每个专家都是一个线性分类器（<a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6215056">来源</a>）：</p>
<p><img src="https://cdn.jsdelivr.net/gh/YangSuoly/Images/RL-notes/202309181225873.png" style="zoom:80%;" /></p>
<ul>
<li><p>训练</p>
<p>可以通过反向传播来学习混合专家模型。根据链式法则，可以得到：</p>
<p><span class="math display">
\nabla f(x) = \sum_{e=1}^E g_e(x) (\nabla (\log g_e(x)) h_{\theta_e}(x)
+ \nabla h_{\theta_e}(x)).
</span></p>
<p>注意到，梯度与 <span class="math inline">g_e(x)</span>
成比例，并且同时更新门控函数和专家。</p></li>
<li><p>节约计算</p>
<p>门控函数 <span class="math inline">g(x) = [g_1(x), \dots,
g_E(x)]</span> 对于每个专家都是非零的。例如： <span
class="math display">
g(x) = [0.04, 0.8, 0.01, 0.15].
</span></p>
<ul>
<li><p>专家的混合不会节省任何计算，因为前向传播仍然需要评估每个专家，而反向传播也必须接触每个专家。</p></li>
<li><p>如果将门控函数 <span class="math inline">g(x) = [g_1(x), \dots,
g_E(x)]</span> 近似为 <span class="math inline">\tilde g(x) = [\tilde
g_1(x), \dots, \tilde
g_E(x)]</span>，其中大多数专家都是零。因此，在前向和反向传播时，只需要使用非零
<span class="math inline">\tilde g_e(x)</span> 的专家 <span
class="math inline">e</span>。</p></li>
</ul>
<p>例如，我们可以选取值排名前两位（top 2）的专家，并重新规范化： <span
class="math display">
\tilde g(x) = [0, 0.84, 0, 0.16].
</span></p></li>
<li><p>平衡专家</p>
<ul>
<li>只有所有专家都参与进来，混合专家才有效。</li>
<li>如果只有一个专家处于活跃状态（例如，<span class="math inline">g(x) =
[0, 1, 0, 0]</span>)，那么这就是浪费。</li>
<li>如果一直处于这种状态，那么未使用的专家的梯度将为零，因此他们将不会收到任何梯度并得到改善。</li>
<li>因此，使用混合专家的主要考虑因素之一是确保所有专家都能被输入使用。</li>
</ul></li>
<li><p>并行</p>
<ul>
<li>混合专家非常有利于并行。</li>
<li>每个专家都可以放置在不同的机器上。</li>
<li>我们可以在中心节点计算近似门控函数 <span class="math inline">\tilde
g(x)</span>。</li>
<li>然后，我们只要求包含激活专家的机器（稀疏）来处理 <span
class="math inline">x</span>。</li>
</ul></li>
</ul>
<h3 id="sparsely-gated-mixture-of-experts-lepikhin-et-al.-2021">9.1.2
Sparsely-gated mixture of experts (<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2006.16668.pdf">Lepikhin et al.
2021</a>)</h3>
<p>现在，我们考虑如何将混合专家思想应用于语言模型。最简单的解决方法仍是使用
<code>96</code> 层
<code>Transformer</code>，但门控函数以某种方式应用于序列，且只在顶层进行专家的组合。因此，我们将混合专家的思想应用于每个
<code>token</code>和每层的 Transformer block（或隔层使用）</p>
<p>由于前馈层对于每个token是独立的，因此，我们将每个前馈网络转变为混合专家（MoE）前馈网络：
<span class="math display">
\text{MoETransformerBlock}(x_{1:L}) =
\text{AddNorm}(\text{MoEFeedForward},
\text{AddNorm}(\text{SelfAttention}, x_{1:L})).
</span></p>
<p><img src="https://cdn.jsdelivr.net/gh/YangSuoly/Images/RL-notes/202309181646256.png" style="zoom:80%;" /></p>
<p>我们将 top-2 专家的近似门控函数定义如下：</p>
<ul>
<li><p>计算第一个专家：<span class="math inline">e_1 = \arg\max_e
g_e(x)</span>。</p></li>
<li><p>计算第二个专家：<span class="math inline">e_2 = \arg\max_{e \neq
e_1} g_e(x)</span>。</p></li>
<li><p>始终保留第一个专家，并随机保留第二个专家：</p>
<ul>
<li>设 <span class="math inline">p = \min(2 g_{e_2}(x), 1)</span></li>
<li>在概率为 <span class="math inline">p</span> 的情况下，<span
class="math inline">\tilde g_{e_1}(x) = \frac{g_{e_1}(x)}{g_{e_1}(x) +
g_{e_2}(x)}, \tilde g_{e_2}(x) = \frac{g_{e_2}(x)}{g_{e_1}(x) +
g_{e_2}(x)}</span>。对于其他专家<span class="math inline">e \not\in \{
e_1, e_2 \}</span>，<span class="math inline">\tilde g_e(x) =
0</span>。</li>
<li>在概率为 <span class="math inline">1 - p</span> 的情况下，<span
class="math inline">\tilde g_{e_1}(x) = 1</span>。对于 <span
class="math inline">e \neq e_1</span>，$g_e(x) = 0 $。</li>
</ul></li>
</ul>
<p>具体细节如下：</p>
<ol type="1">
<li><p>符号定义</p>
<p>设 <span class="math inline">B</span> 是一个 <code>batch</code> 中的
<code>token</code> 数量（在所有序列中，通常在百万数量级），<span
class="math inline">E</span> 是专家数目（通常在千数量级），<span
class="math inline">x_1, \dots, x_B</span> 为一个 <code>batch</code>
中的 <code>token</code>。</p></li>
<li><p>平衡专家</p>
<p>设 <span class="math inline">c_e = \sum_{i=1}^B \mathbf{1}[\tilde
g_e(x_i) &gt; 0]</span> 是专家 <span class="math inline">e</span>
被选中的次数（Note：处理完一个 <code>batch</code> 后，<span
class="math inline">\sum_e c_e = B</span>）。</p>
<ul>
<li><p>如果所有专家都是 <strong>平衡</strong> 的，那么<span
class="math inline">c_e = \frac{B}{E}</span>。</p></li>
<li><p><strong>溢出</strong>：如果 <span class="math inline">c_e &gt; 2
\frac{B}{E}</span>，则设 <span class="math inline">f(x) =
x</span>（带残差的旁路），其中 <code>2</code> 是容量系数。</p></li>
<li><p><strong>辅助损失</strong>：我们期望 <span class="math inline">c =
[c_1, \dots, c_E]</span> 接近均匀分布。</p></li>
</ul>
<p>我们可以惩罚 <span class="math inline">\|c\|_2^2 = \sum_{e=1}^E
c_e^2</span>，但这是不可微分的。</p>
<ul>
<li><p>定义 <span class="math inline">m_e = \sum_{i = 1}^B
g_e(x_i)</span>（ <span class="math inline">c_e</span>
的软版本)。</p></li>
<li><p>在目标函数中添加 <span
class="math inline">\text{load-balancing-loss} = \sum_{e=1}^E m_e
c_e</span>。这样，通过 <span class="math inline">m_e</span>
的梯度将为非零。</p></li>
<li><p><span class="math inline">\text{loss} =
\text{negative-log-likelihood} + \lambda
\text{load-balancing-loss}.</span></p>
<p>例如，取 <span class="math inline">\lambda =
\frac{0.01}{B}</span>。</p></li>
</ul></li>
<li><p>示例</p>
<p>下面是一个<span class="math inline">B=2</span> 个
<code>token</code>，<span class="math inline">E=4</span> 个专家的例子：
<span class="math display">
g(x_1) = [0.2, 0.6, 0.1, 0.1] \Rightarrow \tilde g(x_1) = [0.25, 0.75,
0, 0] \\
g(x_2) = [0.1, 0.6, 0.2, 0.1] \Rightarrow \tilde g(x_2) = [0, 0.75,
0.25, 0]
</span></p>
<p>统计为 <span class="math display">
c = [1, 2, 1, 0] \quad\quad\quad\quad m = [0.3, 1.2, 0.3, 0.2]
</span></p>
<p>也就是说，我们会尝试降低专家 <span class="math inline">2</span>
的权重，避免其被过度使用。</p></li>
</ol>
<h3 id="switch-transformer-fedus-et-al.-2021">9.1.3 Switch Transformer
(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2101.03961.pdf">Fedus et al.
2021</a>)</h3>
<p>定义近似门控函数 <span class="math inline">\tilde g(x)</span>
只有一个专家（获得更多稀疏性）。</p>
<p>技巧：</p>
<ul>
<li>将 <code>FP32</code> 训练替换成 <code>FP16</code></li>
<li>使用的较小参数进行初始化</li>
<li>专家 <code>dropout</code></li>
<li>专家并行</li>
</ul>
<p>训练了一个1.6万亿参数模型，与
<code>T5-XXL</code>（110亿参数）相比，训练速度提高了4倍</p>
<h3
id="balanced-assignment-of-sparse-experts-base-layers-lewis-et-al.-2021">9.1.4
Balanced Assignment of Sparse Experts (BASE) layers (<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.16716.pdf">Lewis et al., 2021</a>)</h3>
<p><code>BASE</code> 将近似门控函数 <span class="math inline">\tilde
g(x)</span> 定义为对 <code>batch</code> 中的所有 <code>token</code>
进行联合优化的结果。它为每个<code>token</code> 分配 <code>1</code>
名专家，但负载平衡是一种约束，而不是软惩罚。</p>
<p>定义 <span class="math inline">a = [a_1, \dots, a_B] \in \{1, \dots,
E\}^B</span> 作为联合分配向量。 <span class="math display">
\begin{align*}
&amp;\text{maximize} \sum_{i = 1}^B w_{a_i} \cdot x_i \\
&amp;\text{subject to}\quad \forall e: \sum_{i=1}^B \mathbf{1}[a_i = e]
= \frac{B}{E}.
\end{align*}
</span>
这是一个可以有效求解的线性方程，在实践中，我们将线性方程并行化。在测试时，只需选择top
1的专家即可。</p>
<ul>
<li><p>实验设置</p>
<ul>
<li>Sparsely gated MoE (top-2 experts): 52.5B 参数</li>
<li>Switch Transformer (top-1 expert): 52.5B 参数</li>
<li>BASE (1 jointly optimized expert): 44.4B 参数 (1.3B shared 参数,
335M x 128 expert 参数)</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/YangSuoly/Images/RL-notes/202309181821233.png" style="zoom:80%;" /></p>
<p><code>BASE</code> 需要更多的计算来优化 <span
class="math inline">a</span>，但更稳定。</p></li>
<li><p>总结和下一步工作</p>
<ul>
<li>Switch Transformer（谷歌）使用了 top-1 专家。</li>
<li>BASE（Facebook）为每个 <code>token</code> 分配 <span
class="math inline">1</span> 名专家，但进行了联合优化。</li>
</ul>
<p>这两个模型的性能都无法与 <code>GPT-3</code> 可比。虽然谷歌和
<code>Facebook</code> 都发布了两个最新的高性能 <code>MoE</code>
语言模型，它们的性能确实与 <code>GPT-3</code>
可比。但有趣的是，它们仍然基于最初简单的top-2专家：</p>
<ul>
<li>谷歌的 <code>GLaM</code></li>
<li>来自Facebook的 <code>FacebookMoE</code></li>
</ul></li>
</ul>
<h3 id="generalist-language-model-glam-du-et-al.-2021">9.1.5 Generalist
Language Model (GLaM) (<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.06905.pdf">Du
et al. 2021</a>)</h3>
<p>规格：</p>
<ul>
<li>1.2万亿个参数（GPT-3有1750亿个参数）</li>
<li>64个专家，64层，32K个隐藏单元</li>
<li>每个token激活95B（1.2T的8%）的参数</li>
</ul>
<p>其他：</p>
<ul>
<li>创建了共有 1.6 万亿个 <code>token</code> 的新数据集（GLaM
dataset），来源包括网页、论坛、书籍、新闻等。</li>
<li>相对位置编码、门控线性单元、GeLU激活函数、RMSNorm（非
LayerNorm）</li>
<li>如果遇到 NaN/Inf，跳过权重更新/回滚到早期检查点。</li>
<li>通过仔细实施上述技巧，我们观察到，稀疏激活的模型在各个尺度上的训练都变得相当稳定。</li>
</ul>
<p><strong>结果</strong>：在与 <code>GPT-3</code>
相同的基准上进行评估（开放域问答、阅读理解、SuperGLUE等）。与
<code>GPT-3</code>
相比，训练成本仅为1/3，且实现了更好的0-shot和1-shot性能（尤其是在知识密集型任务中的性能）</p>
<p>注：他们没有在GPT-3更强的few-shot中进行评估。</p>
<p><img src="https://cdn.jsdelivr.net/gh/YangSuoly/Images/RL-notes/202309190852239.png" style="zoom:80%;" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/YangSuoly/Images/RL-notes/202309190852619.png" style="zoom:80%;" /></p>
<p><strong>示例：</strong></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="emphasis">*The assistant went to work. &#123;She brought her boss coffee., She was valued for her input.&#125;*</span></span><br><span class="line"></span><br><span class="line">刻板印象随着模型大小的增加而变得更糟（与GLaM结果相反）。</span><br></pre></td></tr></table></figure>
<h3 id="facebookmoe-artetxe-et-al.-2021">9.1.6 FacebookMoE (<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.10684.pdf">Artetxe et al.,
2021</a>)</h3>
<ul>
<li><p><strong>实验设置：</strong></p>
<ul>
<li><p>训练了一个1.1T参数的模型。</p></li>
<li><p>512名专家（超过GLaM），32层，4096个隐藏单元</p></li>
<li><p>使用112 billion
token进行训练，来源包括网页、论坛、书籍、新闻等。</p></li>
<li><p>小模型收益更大，模型越大，收益递减</p></li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/YangSuoly/Images/RL-notes/202309190854778.png" style="zoom:80%;" /></p>
<p><a
target="_blank" rel="noopener" href="https://stereoset.mit.edu/explore/dev/">StereoSet</a>上的结果：</p></li>
<li><p><strong>示例</strong>：</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="emphasis">*The assistant went to work. &#123;She brought her boss coffee., She was valued for her input.&#125;*</span></span><br><span class="line"></span><br><span class="line">刻板印象随着模型大小的增加而变得更糟（与GLaM结果相反）。</span><br></pre></td></tr></table></figure>
<p><img src="https://cdn.jsdelivr.net/gh/YangSuoly/Images/RL-notes/202309190922402.png" style="zoom:80%;" /></p></li>
</ul>
<h3 id="decentralized-mixture-of-experts-ryabinin-gusev-2020">9.1.7
Decentralized mixture-of-experts (<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2002.04013.pdf">Ryabinin &amp; Gusev,
2020</a>)</h3>
<ul>
<li><p><strong>动机</strong></p>
<p>到目前为止，混合专家纯粹是中心机构（如谷歌或Facebook）从扩大大语言模型的角度出发的。然而，混合专家自然地指示了一种更激进的权利下放。为训练
<code>GPT-3</code>，<a
target="_blank" rel="noopener" href="https://blogs.microsoft.com/ai/openai-azure-supercomputer/">Azure超级计算机集群</a>
耗资2.5亿美元。</p>
<p>那么如何利用<a
target="_blank" rel="noopener" href="https://lisbdnet.com/how-many-computers-are-there-in-the-world/">数以亿计</a>的消费
PC 呢？</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://foldingathome.org/">Folding@Home</a>
是一个志愿者计算项目，利用世界各地的志愿者捐赠计算机进行分子动力学模拟。</li>
<li>2020年4月，Folding@Home有70万人捐赠了产生2.43
exaFLOP（GPT-3需要350千兆FLOP）（<a
target="_blank" rel="noopener" href="https://www.sciencealert.com/so-many-people-are-running-folding-home-that-it-s-created-the-world-s-biggest-supercomputer">文章</a>）。</li>
<li>主要区别在于分子动力学模拟计算量大，不需要网络带宽。</li>
</ul></li>
<li><p><strong>分布式哈希表</strong>：</p>
<ul>
<li><span class="math inline">N</span> 个节点</li>
<li>单个节点需要与其他 <span class="math inline">O(\log N)</span>
节点通信</li>
<li>使用 Kademlia DHT 协议（被 BitTorrent 和以太坊使用）</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/YangSuoly/Images/RL-notes/202309191001403.png" style="zoom:80%;" /></p></li>
<li><p><strong>论文实验</strong></p>
<ul>
<li>选取 top-4 的专家（共256名专家）</li>
<li>每个专家都是一个 Transformer 层</li>
<li>在4个 GPU 上训练了一个小型 Transformer LM</li>
</ul></li>
</ul>
<h3 id="总结">9.1.8 总结</h3>
<ul>
<li>混合专家：起源于将不同专家应用于不同输入的经典理念</li>
<li>允许训练更大的语言模型（1.1万亿个参数）</li>
<li>与稠密 <code>Transformer</code>
模型相比，每个输入的效率高得多（<code>FLOP</code> 更少）</li>
<li>效果难以比较：在相同规模上，直接比较仍然具有挑战性（<code>GPT-3</code>
与 <code>GLaM</code> 与 <code>FacebookMoE</code>）</li>
<li>对权力下放的重大影响</li>
</ul>
<h2 id="基于检索的模型">9.2 基于检索的模型</h2>
<p>现在，我们转向基于检索的（或检索增强的、记忆增强的模型）语言模型，，它可以帮助我们突破稠密
<code>Transformer</code> 的缩放上限。</p>
<h3 id="编码器-解码器">9.2.1 编码器-解码器</h3>
<p>首先，回顾使用编码器-解码器框架的序列到序列任务： <span
class="math display">
\text{input } x \quad\Rightarrow\quad \text{output } y
</span> 示例（开放问答）：</p>
<ul>
<li>输入 <span class="math inline">x</span>：What is the capital of
Canada?</li>
<li>输出 <span class="math inline">y</span>：Ottawa</li>
</ul>
<p>回想一下，<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1910.13461.pdf">BART</a> 和
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1910.10683.pdf">T5</a>
是编码器-解码器模型的代表： <span class="math display">
p(y \mid x)
</span> 其使用去噪目标函数进行训练。例如：</p>
<ul>
<li>输入 <span class="math inline">x</span>：Thank you <span
class="math inline">&lt;X&gt;</span> me to your party <span
class="math inline">&lt;Y&gt;</span> week.</li>
<li>输出 <span class="math inline">y</span>：<span
class="math inline">&lt;X&gt;</span> for inviting <span
class="math inline">&lt;Y&gt;</span> last</li>
</ul>
<h3 id="检索方法">9.2.2 检索方法</h3>
<p>假设我们有一个存储库 <span
class="math inline">S</span>，它是一组序列（通常是文档或段落）的集合。
<span class="math display">
S = \{ \text{Why is the...}, \text{Thanks for}, ..., \text{The
quick...}, \text{Stanford...} \}.
</span> 基于检索的模型直观的生成过程：</p>
<ul>
<li>基于输入 <span class="math inline">x</span>，检索相关序列<span
class="math inline">z</span>。</li>
<li>给定检索序列 <span class="math inline">z</span> 和输入 <span
class="math inline">x</span>，生成输出 <span
class="math inline">y</span>。</li>
</ul>
<p>示例（开放问答）：</p>
<ul>
<li>输入 <span class="math inline">x</span>：What is the capital of
Canada?</li>
<li>检索 <span class="math inline">z</span>：Ottawa is the capital city
of Canada.</li>
<li>输出 <span class="math inline">y</span>：Ottawa</li>
</ul>
<p>最近邻是最常用的一种检索方法：</p>
<ul>
<li><span class="math inline">S</span> 是训练集。</li>
<li>检索 <span class="math inline">(x&#39;,y&#39;) \in S</span>，使得
<span class="math inline">x&#39;</span> 和 <span
class="math inline">x</span> 最相似。</li>
<li>生成 <span class="math inline">y = y&#39;</span>。</li>
</ul>
<h3 id="retrieval-augmented-generation-rag-lewis-et-al.-2020">9.2.3
Retrieval-augmented generation (RAG) (<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.11401.pdf">Lewis et al., 2020</a>)</h3>
<p><img src="https://cdn.jsdelivr.net/gh/YangSuoly/Images/RL-notes/202309191020000.png" style="zoom:80%;" /></p>
<p>形式上，<code>RAG</code> 模型定义如下： <span class="math display">
(y \mid x) = \sum_{z \in S} \underbrace{p(z \mid x)}_\text{retriever}
\underbrace{p(y \mid z, x)}_\text{generator}.
</span> 在实践中，<span class="math inline">\sum_{z \in S}</span> 由前
<span class="math inline">k</span> 个代替（类似于为混合专家选择前 <span
class="math inline">1</span> 个或 <span class="math inline">2</span>
个专家）。</p>
<ul>
<li><p><strong>检索器</strong>：</p>
<p>Dense Passage Retrieval (DPR)** (<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2004.04906.pdf">Karpukhin et al.,
2020</a>)</p>
<p><span class="math display">
p(z \mid x) = \frac{\exp(\text{BERT}_\text{d}(z) \cdot
\text{BERT}_\text{q}(x))}{\sum_{z&#39; \in S}
\exp(\text{BERT}_\text{d}(z&#39;) \cdot \text{BERT}_\text{q}(x))}.
</span></p>
<p>这里以用维基百科文章的标题来检索段落为例。使用 <code>QA</code>
数据集（如 NaturalQuestions、TriviQA 等）的 query、正例、负例 <span
class="math inline">(q, p^+, p^-_1, \dots, p^-_n)</span>
来训练模型：</p>
<ul>
<li><p>负例：随机或者使用 <code>BM25</code>
检索出的不包含答案的段落</p></li>
<li><p>推理：使用 <a
target="_blank" rel="noopener" href="https://github.com/facebookresearch/faiss">FAISS</a>（Facebook AI
相似性搜索）</p></li>
</ul></li>
<li><p><strong>生成器</strong>：</p>
<p><span class="math display">
p(y \mid z, x) = p(y \mid \text{concat}(z, x)).
</span></p>
<p>使用 BART-large（400M参数），其中输入为检索出的段落<span
class="math inline">z</span>和输入 <span
class="math inline">x</span>。</p></li>
<li><p><strong>训练</strong>：</p>
<ul>
<li><p>用 BART、DPR（用BERT初始化）初始化</p></li>
<li><p>训练 <span class="math inline">\text{BART}</span> 和 <span
class="math inline">\text{BERT}_\text{q}</span></p></li>
</ul></li>
<li><p><strong>实验：</strong></p>
<p>在Jeopardy问题生成任务上，输入Hemingway的检索结果：</p>
<p><img src="https://cdn.jsdelivr.net/gh/YangSuoly/Images/RL-notes/202309191025429.png" style="zoom:80%;" /></p>
<p>实验结果表明，优于非检索方法。</p></li>
</ul>
<h2 id="总结-1">9.3 总结</h2>
<ul>
<li>为了扩大模型规模，需要改进稠密 <code>Transformer</code>。</li>
<li>混合专家和基于检索的方法相结合更有效。</li>
<li>如何设计更好的、可扩展的体系结构仍然是一个悬而未决的问题。</li>
</ul>
<h2 id="reference">9.4 Reference</h2>
<h3 id="混合专家模型-1">混合专家模型</h3>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1701.06538.pdf">Outrageously Large
Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a>.
<em>Noam M. Shazeer et al.</em>. ICLR 2017. Trains 137 billion parameter
model; mixture of experts (1000 experts) applied convolutionally between
LSTM layers.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2006.16668.pdf">GShard: Scaling Giant
Models with Conditional Computation and Automatic Sharding</a>.
<em>Dmitry Lepikhin et al.</em>. ICLR 2020. Trains Transformer for
neural machine translation (100 languages) with 600 billion parameters.
Use top-2 experts.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2101.03961.pdf">Switch Transformers:
Scaling to Trillion Parameter Models with Simple and Efficient
Sparsity</a>. <em>W. Fedus et al.</em>. 2021. Trains language model, 4x
speedup over T5-XXL (13 billion parameters). Use top-1 expert.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.06905.pdf">GLaM: Efficient
Scaling of Language Models with Mixture-of-Experts</a>. <em>Nan Du et
al.</em>. 2021. Trains 1.2 trillion parameter model, 64 experts. Use
top-2 experts. Also creates new dataset.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.16716.pdf">BASE Layers:
Simplifying Training of Large, Sparse Models</a>. <em>M. Lewis et
al.</em>. ICML 2021. Solve optimization problem for token-to-expert
allocation to balance allocation. Trains 110 billion parameter
model.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.10684.pdf">Efficient Large Scale
Language Modeling with Mixtures of Experts</a>. <em>Mikel Artetxe et
al.</em>. 2021. Trains 1.1 trillion parameter models. Use top-2 experts
(512 experts).</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2002.04013.pdf">Towards Crowdsourced
Training of Large Neural Networks using Decentralized
Mixture-of-Experts</a>. <em>Max Ryabinin, Anton I. Gusev</em>. NeurIPS
2020.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.10207.pdf">Distributed Deep
Learning in Open Collaborations</a>. <em>Michael Diskin et al.</em>.
2021.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.14397.pdf">Dense-to-Sparse Gate
for Mixture-of-Experts</a>. <em>Xiaonan Nie et al.</em>. 2021.</li>
</ul>
<h3 id="基于检索的模型-1">基于检索的模型</h3>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2002.08909.pdf">REALM:
Retrieval-Augmented Language Model Pre-Training</a>. <em>Kelvin Guu et
al.</em>. 2020. Introduces <strong>REALM</strong>.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.11401.pdf">Retrieval-Augmented
Generation for Knowledge-Intensive NLP Tasks</a>. <em>Patrick Lewis et
al.</em>. NeurIPS 2020. Introduces <strong>RAG</strong>.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.04426.pdf">Improving language
models by retrieving from trillions of tokens</a>. <em>Sebastian
Borgeaud et al.</em>. 2021. Introduces <strong>RETRO</strong>.</li>
</ul>

    </div>

    
    
    

    <div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">
            -------------This blog is over!
            <i class="fa fa-eye"></i>
            Thanks for your reading-------------
        </div>
    
</div>
      
    </div>
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>YangSu
  </li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="http://yangsuoly.com/2023/09/16/Large-lm-7-Mixed-Framework/" title="Large-lm-7-Mixed-Framework">http://yangsuoly.com/2023/09/16/Large-lm-7-Mixed-Framework/</a>
  </li>
</ul>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/Tags/LM/" rel="tag"><i class="fa fa-tag"></i> LM</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/09/16/Large-lm-6-Training/" rel="prev" title="Large-lm-6-Training">
      <i class="fa fa-chevron-left"></i> Large-lm-6-Training
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/09/19/Large-lm-8-Adaptation/" rel="next" title="Large-lm-8-Adaptation">
      Large-lm-8-Adaptation <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div id="music163player">
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1317956275&auto=0&height=66"></iframe>
      </iframe>
    </div>
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%96%B0%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="nav-text">9. 新的模型架构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B"><span class="nav-text">9.1 混合专家模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86"><span class="nav-text">9.1.1 基础知识</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sparsely-gated-mixture-of-experts-lepikhin-et-al.-2021"><span class="nav-text">9.1.2
Sparsely-gated mixture of experts (Lepikhin et al.
2021)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#switch-transformer-fedus-et-al.-2021"><span class="nav-text">9.1.3 Switch Transformer
(Fedus et al.
2021)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#balanced-assignment-of-sparse-experts-base-layers-lewis-et-al.-2021"><span class="nav-text">9.1.4
Balanced Assignment of Sparse Experts (BASE) layers (Lewis et al., 2021)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#generalist-language-model-glam-du-et-al.-2021"><span class="nav-text">9.1.5 Generalist
Language Model (GLaM) (Du
et al. 2021)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#facebookmoe-artetxe-et-al.-2021"><span class="nav-text">9.1.6 FacebookMoE (Artetxe et al.,
2021)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#decentralized-mixture-of-experts-ryabinin-gusev-2020"><span class="nav-text">9.1.7
Decentralized mixture-of-experts (Ryabinin &amp; Gusev,
2020)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-text">9.1.8 总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%A3%80%E7%B4%A2%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="nav-text">9.2 基于检索的模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="nav-text">9.2.1 编码器-解码器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A3%80%E7%B4%A2%E6%96%B9%E6%B3%95"><span class="nav-text">9.2.2 检索方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#retrieval-augmented-generation-rag-lewis-et-al.-2020"><span class="nav-text">9.2.3
Retrieval-augmented generation (RAG) (Lewis et al., 2020)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-1"><span class="nav-text">9.3 总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reference"><span class="nav-text">9.4 Reference</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B-1"><span class="nav-text">混合专家模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%A3%80%E7%B4%A2%E7%9A%84%E6%A8%A1%E5%9E%8B-1"><span class="nav-text">基于检索的模型</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="YangSu"
      src="/images/YangSu.jpg">
  <p class="site-author-name" itemprop="name">YangSu</p>
  <div class="site-description" itemprop="description">A blog for recording learning notes...</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">78</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/Categories/">
          
        <span class="site-state-item-count">40</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/Tags/">
          
        <span class="site-state-item-count">49</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/YangSuoly" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;YangSuoly" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://gitee.com/yangsuoly" title="Gitee → https:&#x2F;&#x2F;gitee.com&#x2F;yangsuoly" rel="noopener" target="_blank"><i class="fab fa-github-square fa-fw"></i>Gitee</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/64518717" title="B 站 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;64518717" rel="noopener" target="_blank"><i class="fa fa-play-circle fa-fw"></i>B 站</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Related links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.58pic.com/u/19637930/" title="https:&#x2F;&#x2F;www.58pic.com&#x2F;u&#x2F;19637930&#x2F;" rel="noopener" target="_blank">千图网</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">YangSu</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>









<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>


<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'http://yangsuoly.com/2023/09/16/Large-lm-7-Mixed-Framework/',]
      });
      });
  </script>

  
  <script id="ribbon" src="js/canvas-ribbon.js"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/22.2017.cba-normal.model.json"},"display":{"position":"right","width":300,"height":450},"mobile":{"show":false},"react":{"opacity":0.7},"log":false});</script></body>
</html>
